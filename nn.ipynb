{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nn.ipynb`\n",
    "This file aims to use a neural network to solve the multiple-peaks problem. There is information in the peaks and positions of the nodes, but we haven't been able to draw it out yet. Maybe a neural network can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN specs\n",
    "* Input\n",
    "    * Positions of N nodes\n",
    "    * Peaks of N-1 MIs, the peak closest to zero\n",
    "    * Period: avg period of each signal using MI between signal and self\n",
    "    * Total: 3N inputs\n",
    "* Output\n",
    "    * N-1 peaks/time delays\n",
    "\n",
    "More details below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liberties (assumptions) taken:\n",
    "* When finding the period, you need an upper bound. This could be deduced using the speed and the width of the space, but I am just using a fraction of the signal length. I happen to know that one fifth of the length of the signal (`2000 // 5 = 400`, or `1600 // 6 = 320`) always contains the period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other notes:\n",
    "* This is the first time I am implementing a randomization of the target location in FN data, of a sort. There will be a bounding box with a minimum size of 50 in each direction, where all the target nodes have to live inside. This way, we do not need to regenerate the FN data each time, but the target is still at a random position relative to the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from WSN import *\n",
    "import mutual_information as mtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtin.default_fn_equ_params[\"dt\"] = 0.01\n",
    "mtin.default_fn_equ_params[\"T\"] = 20_000\n",
    "mtin.default_fn_equ_params['stim'] = [[[250, 350], [45, 65], [45, 65]]]\n",
    "start_frame = int(40 / mtin.default_fn_equ_params[\"dt\"])\n",
    "mtin.default_fn_equ = FHN(**mtin.default_fn_equ_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mtin.default_fn_sol is None:\n",
    "    mtin.default_fn_sol = np.load(\"default_fn_sol_dt_0.01.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtin.solve_default_fn_equ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"default_fn_sol_dt_0.01.npy\", mtin.default_fn_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signals and find all peaks\n",
    "def get_fn_peaks_inside_period(self: WSN, r0=None, use_mst=False):\n",
    "    if r0 is None:\n",
    "        r0 = np.array([55, 55])\n",
    "    results = self.transmit_continuous(signal_type=\"fn\", start_frame=start_frame)\n",
    "    assert np.all(np.array([result[0] for result in results]) == self.nodes)\n",
    "\n",
    "    self.printv(\"Finding period\")\n",
    "    avg_period = 0\n",
    "    for i, sigi in results:\n",
    "        # This is arbitrary, but I know this will work for this case\n",
    "        max_tau = len(sigi) // 5\n",
    "        shifts = np.arange(-max_tau, max_tau, 1)\n",
    "        mis = mi_shift(np.transpose([sigi, sigi]), shifts)\n",
    "\n",
    "        mis = np.array(mis)\n",
    "        max_peak = mis.max()\n",
    "        peaks, _ = find_peaks(mis)\n",
    "        peaks = peaks[np.argsort(mis[peaks])]\n",
    "        peaks = peaks[-5:]\n",
    "        peaks.sort()\n",
    "        period1 = peaks[1] - peaks[0]\n",
    "        period2 = peaks[2] - peaks[1]\n",
    "        period = (period1 + period2) / 2\n",
    "        dt = mtin.default_fn_equ_params[\"dt\"]\n",
    "        period *= dt\n",
    "        avg_period += period\n",
    "    avg_period /= len(results)\n",
    "    self.printv(\"Period found to be\", avg_period)\n",
    "\n",
    "    all_peaks = []   # [(rn, rm, p, p0), ...]\n",
    "    pair_to_ind = {} # {(n, m): i}\n",
    "\n",
    "    # For now, tree must be a list so that the neural network\n",
    "    # sees the peaks in the same order every time\n",
    "    if use_mst:\n",
    "        tree = list(self.find_MST())\n",
    "    else:\n",
    "        tree = [\n",
    "            (None, 0, i)\n",
    "            for i in range(1, len(self.nodes))\n",
    "        ]\n",
    "    for _, i, j in tree:\n",
    "        ri, sigi = results[i]\n",
    "        rj, sigj = results[j]\n",
    "        dt = mtin.default_fn_equ_params[\"dt\"]\n",
    "        max_tau = int((2 * avg_period) / dt)\n",
    "        shifts = np.arange(-max_tau, max_tau, 1)\n",
    "        mis = mtin.mi_shift(np.transpose([sigi, sigj]), shifts)\n",
    "\n",
    "        mis = np.array(mis)\n",
    "        max_peak = mis.max()\n",
    "        inclusion_factor = 0.5\n",
    "        peaks, _ = find_peaks(mis, height=max_peak * inclusion_factor)\n",
    "        if self.verbose and np.random.uniform(0, 1) < 0.1:\n",
    "            plt.plot(shifts, mis)\n",
    "            plt.show(block=False)\n",
    "        if len(peaks) == 0:\n",
    "            plt.plot(shifts * mtin.default_fn_equ_params[\"dt\"], mis)\n",
    "            plt.show(block=False)\n",
    "            p = (dist(ri, r0) - dist(rj, r0)) / self.c\n",
    "            raise ValueError(\n",
    "                f\"No peaks found between nodes {i} and {j} \"\n",
    "                f\"at positions {ri} and {rj}.\\n\"\n",
    "                f\"The peak should be at time {p}.\"\n",
    "            )\n",
    "        peaks = shifts[peaks]\n",
    "        # p0 = peak closest to 0\n",
    "        p0 = peaks[np.argmin(np.abs(peaks))]\n",
    "\n",
    "        # p = ideal peak\n",
    "        p = (dist(ri, r0) - dist(rj, r0)) / self.c\n",
    "        pair_to_ind[(i, j)] = len(all_peaks)\n",
    "        all_peaks.append((ri, rj, p, p0))\n",
    "    return all_peaks, pair_to_ind, avg_period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[setattr(WSN, attr, globals()[attr]) for attr in (\n",
    "    \"get_fn_peaks_inside_period\",\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create testing and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input format:\n",
    "* `x0, y0, x1, y1, ...`   (normalized by `wsn.size`)\n",
    "* `p0, p1, p2, ...`       (normalized by period)\n",
    "* `period`\n",
    "\n",
    "Output format:\n",
    "* `p0, p1, p2, ...`       (normalized by period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "c = 1.6424885622140555\n",
    "wsn = WSN(100, N, D=142, std=0, c=c, verbose=False)\n",
    "wsn.reset_anchors(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_and_output(wsn: WSN):\n",
    "    all_peaks, pair_to_ind, period = wsn.get_fn_peaks_inside_period()\n",
    "    # Input format:\n",
    "    # [\n",
    "    #   wsn.nodes.flatten() -- len = 2N,\n",
    "    #   peaks -- len = N-1,\n",
    "    #   period -- len = 1\n",
    "    # ]\n",
    "\n",
    "    # Output format:\n",
    "    # peaks -- len = N-1\n",
    "    input = []\n",
    "    output = []\n",
    "    for ri, rj, p, p0 in all_peaks:\n",
    "        input.append(p0 / period)\n",
    "        output.append(p / period)\n",
    "    input.append(period)\n",
    "    input = np.concatenate((wsn.nodes.flatten() / 100, input))\n",
    "    output = np.array(output)\n",
    "\n",
    "    return input, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]d:\\Repositories\\WSN-Localization\\mutual_information.py:105: RuntimeWarning: divide by zero encountered in log\n",
      "  mi = -0.5 * np.log(np.linalg.det(cor))\n",
      "100%|██████████| 100/100 [46:51<00:00, 28.12s/it]\n"
     ]
    }
   ],
   "source": [
    "data_size = 100\n",
    "# input_shape = (data_size, 3 * N)\n",
    "# output_shape = (data_size, N - 1)\n",
    "input = [0] * data_size\n",
    "output = [0] * data_size\n",
    "for j in tqdm(range(data_size)):\n",
    "    error = True\n",
    "    while error:\n",
    "        error = False\n",
    "        try:\n",
    "            wsn.reset_nodes()\n",
    "            i, o = get_input_and_output(wsn)\n",
    "            input[j] = i\n",
    "            output[j] = o\n",
    "        except ValueError as e:\n",
    "            print(f\"ValueError: {e}\")\n",
    "            error = True\n",
    "input = np.array(input)\n",
    "output = np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 float64 float64 float64 float64 float64\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_size1, data_size2, data_size3):\n",
    "    input1 = np.load(f\"nn_saves/copy/input{data_size1}.npy\", allow_pickle=True)\n",
    "    output1 = np.load(f\"nn_saves/copy/output{data_size1}.npy\", allow_pickle=True)\n",
    "    input2 = np.load(f\"nn_saves/copy/(2) input{data_size2}.npy\", allow_pickle=True)\n",
    "    output2 = np.load(f\"nn_saves/copy/(2) output{data_size2}.npy\", allow_pickle=True)\n",
    "    input3 = np.load(f\"nn_saves/copy/(3) input{data_size3}.npy\", allow_pickle=True)\n",
    "    output3 = np.load(f\"nn_saves/copy/(3) output{data_size3}.npy\", allow_pickle=True)\n",
    "    input2 = input2[:data_size2]\n",
    "    output2 = output2[:data_size2]\n",
    "    input2 = np.array([*input2])\n",
    "    output2 = np.array([*output2])\n",
    "    print(*[obj.dtype for obj in (input1, output1, input2, output2, input3, output3)])\n",
    "    # Corrections for old data\n",
    "    input1 = input1[:, :-1]\n",
    "    output1 = output1[:, :-1]\n",
    "    input2 = input2[:, :-1]\n",
    "    output2 = output2[:, :-1]\n",
    "    input3 = input3[:, :-1]\n",
    "    output3 = output3[:, :-1]\n",
    "    input = np.concatenate((input1, input2, input3))\n",
    "    output = np.concatenate((output1, output2, output3))\n",
    "    return input, output\n",
    "input, output = load_data(2000, 2000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.load(f\"nn_saves/orig/input{data_size}.npy\", allow_pickle=True)\n",
    "output = np.load(f\"nn_saves/copy/output{data_size}.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrections for old data\n",
    "input = np.array([x for x in input[:data_size]])\n",
    "output = np.array([x for x in output[:data_size]])\n",
    "input = input[:, :-1]\n",
    "output = output[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2 = np.load(\"nn_saves/copy/(2) input400.npy\", allow_pickle=True)\n",
    "output2 = np.load(\"nn_saves/copy/(2) output400.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrections for old data\n",
    "input2 = input2[:, :-1]\n",
    "output2 = output2[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 36), (400, 11))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2.shape, output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.concatenate((input, input2), axis=0)\n",
    "output = np.concatenate((output, output2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 36)\n",
      "(6000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)\n",
    "print(output.shape)\n",
    "data_size = len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.67032026e-01  2.55447035e-01  6.84546894e-02  9.17007937e-01\n",
      "  7.08423579e-02  6.83348825e-02  4.96216109e-01  1.74547307e-02\n",
      "  8.79624750e-01  4.96301752e-01  3.06407044e-01  3.28620519e-01\n",
      "  5.20289613e-01  1.18830827e-02  1.34066432e-01  6.09224732e-01\n",
      "  8.44997969e-01  5.18230939e-01  6.72768384e-01  4.89914870e-01\n",
      "  6.28993510e-01  6.15569032e-01  6.06160553e-01  3.81025015e-01\n",
      " -3.84529461e+01 -7.18793633e+01 -9.81848646e+01 -3.93744764e+00\n",
      " -2.20329517e+01 -9.72633343e+01 -4.44010053e+01  1.10583636e+01\n",
      "  2.13627478e+01  4.02122312e+00 -2.93214186e+01  3.10827994e+00]\n",
      "[-1.47154165 -1.84871252 -1.11346063 -0.08680584 -0.06227296 -1.13224162\n",
      " -0.52626109  0.10327194  0.91945981  1.09299102  0.70839955]\n"
     ]
    }
   ],
   "source": [
    "print(input[0])\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = data_size * 8 // 10\n",
    "train_input = input[:num_train_samples]\n",
    "train_output = output[:num_train_samples]\n",
    "test_input = input[num_train_samples:]\n",
    "test_output = output[num_train_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1432"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(3*N,)),\n",
    "    keras.layers.Dense(512, activation=\"linear\"),\n",
    "    keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    keras.layers.Dense(2048, activation=\"relu\"),\n",
    "    keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    keras.layers.Dense(1024, activation=\"relu\"),\n",
    "    keras.layers.Dense(512, activation=\"relu\"),\n",
    "    keras.layers.Dense(N-1, activation=\"linear\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=keras.losses.mean_squared_error, metrics=[\"mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "192/192 [==============================] - 10s 49ms/step - loss: 1.5985 - mean_squared_error: 1.5985 - val_loss: 1.1341 - val_mean_squared_error: 1.1341\n",
      "Epoch 2/10\n",
      "192/192 [==============================] - 10s 51ms/step - loss: 1.0542 - mean_squared_error: 1.0542 - val_loss: 1.0913 - val_mean_squared_error: 1.0913\n",
      "Epoch 3/10\n",
      "192/192 [==============================] - 10s 51ms/step - loss: 1.0424 - mean_squared_error: 1.0424 - val_loss: 1.0689 - val_mean_squared_error: 1.0689\n",
      "Epoch 4/10\n",
      "192/192 [==============================] - 10s 54ms/step - loss: 1.0323 - mean_squared_error: 1.0323 - val_loss: 1.0696 - val_mean_squared_error: 1.0696\n",
      "Epoch 5/10\n",
      "192/192 [==============================] - 10s 52ms/step - loss: 1.0239 - mean_squared_error: 1.0239 - val_loss: 1.0588 - val_mean_squared_error: 1.0588\n",
      "Epoch 6/10\n",
      "192/192 [==============================] - 10s 51ms/step - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.0638 - val_mean_squared_error: 1.0638\n",
      "Epoch 7/10\n",
      "192/192 [==============================] - 10s 51ms/step - loss: 1.0084 - mean_squared_error: 1.0084 - val_loss: 1.0767 - val_mean_squared_error: 1.0767\n",
      "Epoch 8/10\n",
      "192/192 [==============================] - 10s 52ms/step - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 1.1095 - val_mean_squared_error: 1.1095\n",
      "Epoch 9/10\n",
      "192/192 [==============================] - 10s 51ms/step - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.0581 - val_mean_squared_error: 1.0581\n",
      "Epoch 10/10\n",
      "192/192 [==============================] - 10s 51ms/step - loss: 0.9987 - mean_squared_error: 0.9987 - val_loss: 1.0616 - val_mean_squared_error: 1.0616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24caf7d2d60>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_output, validation_data=(test_input, test_output), batch_size=25, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 1s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.00085952,  0.01251677, -0.01614039, -0.05552344, -0.11687545,\n",
       "        -0.10616183, -0.00083554, -0.06193177, -0.045387  , -0.23937139,\n",
       "         0.04486244], dtype=float32),\n",
       " array([ 0.30201001, -0.48214488,  0.30031751,  0.48871973, -0.88692996,\n",
       "        -0.47003263, -1.30997936,  1.41865404, -0.10948085, -0.79822337,\n",
       "        -0.97715453]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(test_input)\n",
    "prediction[0], test_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def get_redundancies():\n",
    "    b = 0\n",
    "    redundancies = np.zeros(len(input), dtype=np.int64)\n",
    "    for j in range(data_size):\n",
    "        for k in range(j):\n",
    "            if np.all(input[j] == input[k]):\n",
    "                redundancies[b] = j\n",
    "                b += 1\n",
    "                break\n",
    "    redundancies = redundancies[:b]\n",
    "    return redundancies\n",
    "def get_redundancy_info():\n",
    "    redundancies = get_redundancies()\n",
    "    # redundancies = np.array(redundancies)\n",
    "    print(len(redundancies))\n",
    "    slices = [[redundancies[0], redundancies[0] + 1]]\n",
    "    for i, j in enumerate(redundancies):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if j == slices[-1][1]:\n",
    "            slices[-1][1] = j + 1\n",
    "        else:\n",
    "            slices.append([j, j + 1])\n",
    "    for i, s in enumerate(slices):\n",
    "        if s[1] == s[0] + 1:\n",
    "            slices[i] = s[0]\n",
    "    print(slices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('vwsn': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a326387514bff8cff46362682c6b5d6706650c507f9d6555fe15603d3afa14e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
